#!/usr/bin/env python3
"""
æ·±å…¥åˆ†æ mAP ä¸ç†æƒ³çš„å¯èƒ½åŸå› 
ä»è®­ç»ƒç»“æœCSVå’Œé…ç½®æ–‡ä»¶ä¸­æå–å…³é”®ä¿¡æ¯
"""
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import numpy as np

matplotlib.use('Agg')

plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False


def analyze_training_issues(csv_path, model_name):
    """åˆ†æè®­ç»ƒè¿‡ç¨‹ä¸­çš„é—®é¢˜"""
    df = pd.read_csv(csv_path)
    df.columns = df.columns.str.strip()

    issues = []
    recommendations = []

    print(f"\n{'='*80}")
    print(f"åˆ†æ {model_name} çš„è®­ç»ƒé—®é¢˜")
    print(f"{'='*80}\n")

    # 1. æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ
    final_train_loss = df['train/box_loss'].iloc[-1] + df['train/cls_loss'].iloc[-1] + df['train/dfl_loss'].iloc[-1]
    df_val_clean = df[df['val/box_loss'].notna() & ~df['val/box_loss'].isin([float('inf'), float('-inf')])]
    final_val_loss = df_val_clean['val/box_loss'].iloc[-1] + df_val_clean['val/cls_loss'].iloc[-1] + df_val_clean['val/dfl_loss'].iloc[-1]

    train_val_gap = final_val_loss - final_train_loss
    print(f"1. è¿‡æ‹Ÿåˆæ£€æŸ¥:")
    print(f"   è®­ç»ƒæŸå¤±: {final_train_loss:.4f}")
    print(f"   éªŒè¯æŸå¤±: {final_val_loss:.4f}")
    print(f"   å·®è·: {train_val_gap:.4f}")

    if train_val_gap > 0.5:
        issues.append("è¿‡æ‹Ÿåˆä¸¥é‡: éªŒè¯æŸå¤±è¿œé«˜äºè®­ç»ƒæŸå¤±")
        recommendations.append("- å¢å¼ºæ•°æ®å¢å¼º (æ›´å¼ºçš„augmentation)")
        recommendations.append("- å¢åŠ  dropout")
        recommendations.append("- å¢åŠ  weight_decay")
        recommendations.append("- ä½¿ç”¨æ›´å¤šçš„è®­ç»ƒæ•°æ®")
        print(f"   âš ï¸  è­¦å‘Š: å­˜åœ¨è¿‡æ‹Ÿåˆ! (å·®è· > 0.5)")
    else:
        print(f"   âœ“ è¿‡æ‹Ÿåˆæƒ…å†µå¯æ§")

    # 2. æ£€æŸ¥æŸå¤±ä¸‹é™è¶‹åŠ¿
    print(f"\n2. æŸå¤±æ”¶æ•›åˆ†æ:")
    last_50_epochs = df.tail(50)
    train_loss_std = last_50_epochs['train/box_loss'].std()
    val_loss_std = df_val_clean.tail(50)['val/box_loss'].std()

    print(f"   æœ€å50ä¸ªepochè®­ç»ƒæŸå¤±æ ‡å‡†å·®: {train_loss_std:.4f}")
    print(f"   æœ€å50ä¸ªepochéªŒè¯æŸå¤±æ ‡å‡†å·®: {val_loss_std:.4f}")

    if train_loss_std < 0.01:
        issues.append("è®­ç»ƒæŸå¤±å·²æ”¶æ•›ä½†mAPè¾ƒä½")
        recommendations.append("- è€ƒè™‘ä»æ›´å¥½çš„é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹")
        recommendations.append("- æ£€æŸ¥æ•°æ®æ ‡æ³¨è´¨é‡")
        recommendations.append("- å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ¨¡å‹ (yolov8l/x)")
        print(f"   âš ï¸  è®­ç»ƒæŸå¤±å·²æ”¶æ•›ï¼Œä½†æ€§èƒ½ä¸ç†æƒ³")
    else:
        print(f"   âœ“ è®­ç»ƒæŸå¤±ä»åœ¨ä¸‹é™ï¼Œå¯ä»¥è®­ç»ƒæ›´å¤šepoch")
        recommendations.append("- å¢åŠ è®­ç»ƒepochæ•° (200-300)")

    # 3. æ£€æŸ¥å„ç±»æŸå¤±çš„æ¯”ä¾‹
    print(f"\n3. æŸå¤±æˆåˆ†åˆ†æ:")
    final_box_loss = df['train/box_loss'].iloc[-1]
    final_cls_loss = df['train/cls_loss'].iloc[-1]
    final_dfl_loss = df['train/dfl_loss'].iloc[-1]

    print(f"   Box Loss: {final_box_loss:.4f} ({final_box_loss/final_train_loss*100:.1f}%)")
    print(f"   Cls Loss: {final_cls_loss:.4f} ({final_cls_loss/final_train_loss*100:.1f}%)")
    print(f"   DFL Loss: {final_dfl_loss:.4f} ({final_dfl_loss/final_train_loss*100:.1f}%)")

    if final_cls_loss / final_train_loss > 0.5:
        issues.append("åˆ†ç±»æŸå¤±å æ¯”è¿‡é«˜")
        recommendations.append("- ç±»åˆ«æ··æ·†ä¸¥é‡ï¼Œæ£€æŸ¥ç›¸ä¼¼ç±»åˆ«çš„æ ‡æ³¨")
        recommendations.append("- å¯èƒ½éœ€è¦å¢åŠ  cls loss weight")
        recommendations.append("- è€ƒè™‘ä½¿ç”¨ focal loss")
        print(f"   âš ï¸  åˆ†ç±»æŸå¤±å æ¯”è¿‡é«˜ (>{50}%)")

    # 4. æ£€æŸ¥ precision å’Œ recall çš„å¹³è¡¡
    print(f"\n4. Precision-Recall å¹³è¡¡:")
    final_precision = df['metrics/precision(B)'].iloc[-1]
    final_recall = df['metrics/recall(B)'].iloc[-1]

    print(f"   Precision: {final_precision:.4f}")
    print(f"   Recall: {final_recall:.4f}")
    print(f"   F1-Score: {2 * (final_precision * final_recall) / (final_precision + final_recall):.4f}")

    if final_precision > final_recall + 0.1:
        issues.append("Recall æ˜æ˜¾ä½äº Precision")
        recommendations.append("- æ¨¡å‹è¿‡äºä¿å®ˆï¼Œæ¼æ£€è¾ƒå¤š")
        recommendations.append("- é™ä½ conf_threshold")
        recommendations.append("- è°ƒæ•´ anchor boxes")
        print(f"   âš ï¸  Recall ä½äº Precisionï¼Œå­˜åœ¨æ¼æ£€é—®é¢˜")
    elif final_recall > final_precision + 0.1:
        issues.append("Precision æ˜æ˜¾ä½äº Recall")
        recommendations.append("- æ¨¡å‹è¿‡äºæ¿€è¿›ï¼Œè¯¯æ£€è¾ƒå¤š")
        recommendations.append("- æé«˜ conf_threshold")
        recommendations.append("- å¢åŠ è´Ÿæ ·æœ¬")
        print(f"   âš ï¸  Precision ä½äº Recallï¼Œå­˜åœ¨è¯¯æ£€é—®é¢˜")
    else:
        print(f"   âœ“ Precision å’Œ Recall ç›¸å¯¹å¹³è¡¡")

    # 5. mAP åˆ†æ
    print(f"\n5. mAP æ€§èƒ½åˆ†æ:")
    final_map50 = df['metrics/mAP50(B)'].iloc[-1]
    final_map50_95 = df['metrics/mAP50-95(B)'].iloc[-1]

    print(f"   mAP@0.5: {final_map50:.4f}")
    print(f"   mAP@0.5:0.95: {final_map50_95:.4f}")
    print(f"   mAPé™å¹…: {(final_map50 - final_map50_95)/final_map50*100:.1f}%")

    if final_map50 - final_map50_95 > 0.15:
        issues.append("mAP@0.5 åˆ° mAP@0.5:0.95 ä¸‹é™æ˜æ˜¾")
        recommendations.append("- å®šä½ç²¾åº¦ä¸è¶³ï¼Œè¾¹ç•Œæ¡†ä¸å¤Ÿå‡†ç¡®")
        recommendations.append("- å¢åŠ  box loss weight")
        recommendations.append("- ä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡è®­ç»ƒ (imgsz=1280)")
        recommendations.append("- æ£€æŸ¥æ ‡æ³¨æ¡†æ˜¯å¦å‡†ç¡®")
        print(f"   âš ï¸  å®šä½ç²¾åº¦é—®é¢˜: mAPåœ¨é«˜IoUé˜ˆå€¼ä¸‹ä¸‹é™æ˜¾è‘—")

    if final_map50 < 0.6:
        issues.append("æ€»ä½“ mAP åä½")
        recommendations.append("- å¯èƒ½æ˜¯æ•°æ®è´¨é‡é—®é¢˜")
        recommendations.append("- æ£€æŸ¥æ•°æ®é›†å¤§å°æ˜¯å¦è¶³å¤Ÿ")
        recommendations.append("- éªŒè¯æ•°æ®å¢å¼ºæ˜¯å¦è¿‡åº¦")
        recommendations.append("- è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹")
        print(f"   âš ï¸  æ•´ä½“æ€§èƒ½åä½ (mAP@0.5 < 0.6)")

    # 6. å­¦ä¹ ç‡æ£€æŸ¥
    print(f"\n6. å­¦ä¹ ç‡è°ƒåº¦:")
    initial_lr = df['lr/pg0'].iloc[0]
    final_lr = df['lr/pg0'].iloc[-1]
    print(f"   åˆå§‹å­¦ä¹ ç‡: {initial_lr:.6f}")
    print(f"   æœ€ç»ˆå­¦ä¹ ç‡: {final_lr:.6f}")
    print(f"   è¡°å‡æ¯”ä¾‹: {final_lr/initial_lr:.4f}")

    # 7. æ—©æœŸè®­ç»ƒé˜¶æ®µåˆ†æ
    print(f"\n7. æ—©æœŸè®­ç»ƒåˆ†æ (å‰30ä¸ªepoch):")
    early_df = df.head(30)
    early_map_improvement = early_df['metrics/mAP50(B)'].iloc[-1] - early_df['metrics/mAP50(B)'].iloc[0]
    print(f"   æ—©æœŸ mAP@0.5 æå‡: {early_map_improvement:.4f}")

    if early_map_improvement < 0.1:
        issues.append("æ—©æœŸè®­ç»ƒé˜¶æ®µæå‡ç¼“æ…¢")
        recommendations.append("- åˆå§‹å­¦ä¹ ç‡å¯èƒ½å¤ªå°ï¼Œå°è¯• lr0=0.02")
        recommendations.append("- warmup epochs å¯èƒ½å¤ªé•¿")
        print(f"   âš ï¸  æ—©æœŸæå‡ç¼“æ…¢ï¼Œå¯èƒ½å­¦ä¹ ç‡è®¾ç½®ä¸å½“")

    return issues, recommendations


def create_diagnostic_plots(csv_path, model_name, output_path):
    """åˆ›å»ºè¯Šæ–­å›¾è¡¨"""
    df = pd.read_csv(csv_path)
    df.columns = df.columns.str.strip()

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle(f'{model_name} - Diagnostic Analysis', fontsize=16, fontweight='bold')

    # 1. Train vs Val Loss
    ax = axes[0, 0]
    df['train_total'] = df['train/box_loss'] + df['train/cls_loss'] + df['train/dfl_loss']
    df_val_clean = df[df['val/box_loss'].notna() & ~df['val/box_loss'].isin([float('inf'), float('-inf')])]
    df_val_clean['val_total'] = df_val_clean['val/box_loss'] + df_val_clean['val/cls_loss'] + df_val_clean['val/dfl_loss']

    ax.plot(df['epoch'], df['train_total'], label='Train', linewidth=2)
    ax.plot(df_val_clean['epoch'], df_val_clean['val_total'], label='Val', linewidth=2)
    ax.fill_between(df_val_clean['epoch'],
                     df_val_clean['val_total'].rolling(10).mean() - df_val_clean['val_total'].rolling(10).std(),
                     df_val_clean['val_total'].rolling(10).mean() + df_val_clean['val_total'].rolling(10).std(),
                     alpha=0.2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Total Loss')
    ax.set_title('Overfitting Check: Train vs Val Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 2. Loss Components Ratio
    ax = axes[0, 1]
    epochs = df['epoch']
    box_ratio = df['train/box_loss'] / df['train_total']
    cls_ratio = df['train/cls_loss'] / df['train_total']
    dfl_ratio = df['train/dfl_loss'] / df['train_total']

    ax.plot(epochs, box_ratio, label='Box Loss %', linewidth=2)
    ax.plot(epochs, cls_ratio, label='Cls Loss %', linewidth=2)
    ax.plot(epochs, dfl_ratio, label='DFL Loss %', linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss Ratio')
    ax.set_title('Loss Component Distribution')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 3. Precision-Recall Curve
    ax = axes[0, 2]
    ax.plot(df['epoch'], df['metrics/precision(B)'], label='Precision', linewidth=2, marker='o', markersize=2, markevery=10)
    ax.plot(df['epoch'], df['metrics/recall(B)'], label='Recall', linewidth=2, marker='s', markersize=2, markevery=10)

    # æ·»åŠ  F1-Score
    f1_score = 2 * (df['metrics/precision(B)'] * df['metrics/recall(B)']) / (df['metrics/precision(B)'] + df['metrics/recall(B)'])
    ax.plot(df['epoch'], f1_score, label='F1-Score', linewidth=2, linestyle='--')

    ax.set_xlabel('Epoch')
    ax.set_ylabel('Metric Value')
    ax.set_title('Precision-Recall Balance')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0, 1])

    # 4. mAP Gap Analysis
    ax = axes[1, 0]
    map_gap = df['metrics/mAP50(B)'] - df['metrics/mAP50-95(B)']
    ax.plot(df['epoch'], map_gap, linewidth=2, color='red')
    ax.fill_between(df['epoch'], 0, map_gap, alpha=0.3, color='red')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('mAP Gap')
    ax.set_title('Localization Accuracy (mAP@0.5 - mAP@0.5:0.95)')
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0.15, color='orange', linestyle='--', label='Threshold (0.15)')
    ax.legend()

    # 5. Learning Rate
    ax = axes[1, 1]
    ax.plot(df['epoch'], df['lr/pg0'], linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Learning Rate')
    ax.set_title('Learning Rate Schedule')
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')

    # 6. mAP Improvement Rate
    ax = axes[1, 2]
    map_diff = df['metrics/mAP50(B)'].diff().rolling(10).mean()
    ax.plot(df['epoch'].iloc[1:], map_diff.iloc[1:], linewidth=2)
    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('mAP Change (smoothed)')
    ax.set_title('mAP Improvement Rate (10-epoch avg)')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nè¯Šæ–­å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    plt.close()


def main():
    models = [
        {
            'csv': '/nas03/yixuh/garbage-classification/models/garbage_yolov8s/results.csv',
            'name': 'YOLOv8s',
            'plot': '/nas03/yixuh/garbage-classification/scripts/visulization/yolov8s_diagnostics.png'
        },
        {
            'csv': '/nas03/yixuh/garbage-classification/models/garbage_yolov8m/results.csv',
            'name': 'YOLOv8m',
            'plot': '/nas03/yixuh/garbage-classification/scripts/visulization/yolov8m_diagnostics.png'
        }
    ]

    all_issues = {}
    all_recommendations = {}

    for model in models:
        issues, recommendations = analyze_training_issues(model['csv'], model['name'])
        all_issues[model['name']] = issues
        all_recommendations[model['name']] = recommendations

        create_diagnostic_plots(model['csv'], model['name'], model['plot'])

    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ç»¼åˆè¯Šæ–­æŠ¥å‘Š")
    print(f"{'='*80}\n")

    for model_name in all_issues.keys():
        print(f"\n{model_name} å‘ç°çš„é—®é¢˜:")
        print("-" * 80)
        if all_issues[model_name]:
            for i, issue in enumerate(all_issues[model_name], 1):
                print(f"{i}. {issue}")
        else:
            print("æœªå‘ç°æ˜æ˜¾é—®é¢˜")

        print(f"\n{model_name} æ”¹è¿›å»ºè®®:")
        print("-" * 80)
        if all_recommendations[model_name]:
            for rec in all_recommendations[model_name]:
                print(f"{rec}")
        else:
            print("æš‚æ— ç‰¹æ®Šå»ºè®®")

    # é€šç”¨å»ºè®®
    print(f"\n{'='*80}")
    print("é€šç”¨æ”¹è¿›å»ºè®® (åŸºäºåƒåœ¾åˆ†ç±»ä»»åŠ¡ç‰¹ç‚¹)")
    print(f"{'='*80}")

    general_recommendations = [
        "\nğŸ“Š æ•°æ®æ–¹é¢:",
        "- æ£€æŸ¥æ•°æ®é›†å¤§å°: åƒåœ¾åˆ†ç±»é€šå¸¸éœ€è¦æ¯ç±»è‡³å°‘500-1000å¼ å›¾ç‰‡",
        "- éªŒè¯æ ‡æ³¨è´¨é‡: æ£€æŸ¥è¾¹ç•Œæ¡†æ˜¯å¦å‡†ç¡®ï¼Œç±»åˆ«æ˜¯å¦æ ‡æ³¨æ­£ç¡®",
        "- ç±»åˆ«å¹³è¡¡: æ£€æŸ¥å„ç±»åˆ«æ ·æœ¬æ•°é‡æ˜¯å¦å‡è¡¡",
        "- æ•°æ®å¤šæ ·æ€§: ç¡®ä¿åŒ…å«ä¸åŒå…‰ç…§ã€è§’åº¦ã€èƒŒæ™¯çš„å›¾ç‰‡",

        "\nğŸ”§ æ¨¡å‹é…ç½®:",
        "- è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ (YOLOv8l æˆ– YOLOv8x)",
        "- å¢åŠ è®­ç»ƒåˆ†è¾¨ç‡ (imgsz=1280)",
        "- è°ƒæ•´ anchor boxes ä»¥é€‚åº”åƒåœ¾ç‰©ä½“çš„å°ºå¯¸åˆ†å¸ƒ",

        "\nğŸ¯ è®­ç»ƒç­–ç•¥:",
        "- å¢åŠ è®­ç»ƒè½®æ•°åˆ° 200-300 epochs",
        "- ä½¿ç”¨ cosine learning rate schedule (cos_lr=True)",
        "- å°è¯•ä¸åŒçš„ optimizer (SGD vs AdamW)",
        "- è°ƒæ•´ batch size (æ›´å¤§çš„ batch size å¯èƒ½æ›´ç¨³å®š)",

        "\nğŸŒˆ æ•°æ®å¢å¼º:",
        "- åƒåœ¾åˆ†ç±»åœºæ™¯å¯èƒ½éœ€è¦æ›´å¼ºçš„é¢œè‰²å¢å¼º",
        "- é€‚å½“å¢åŠ  mosaic å’Œ mixup æ¦‚ç‡",
        "- è€ƒè™‘æ·»åŠ æ¨¡ç³Šã€å™ªå£°ç­‰å¢å¼º",

        "\nğŸ“ˆ å…¶ä»–:",
        "- ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ (COCO dataset)",
        "- å®æ–½ class-weighted loss å¤„ç†ç±»åˆ«ä¸å¹³è¡¡",
        "- å°è¯• test-time augmentation (TTA)",
        "- è¿›è¡Œé”™è¯¯åˆ†æï¼ŒæŸ¥çœ‹å“ªäº›ç±»åˆ«å®¹æ˜“æ··æ·†"
    ]

    for rec in general_recommendations:
        print(rec)

    print(f"\n{'='*80}\n")


if __name__ == '__main__':
    main()
